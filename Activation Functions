import math
def tanh(x):
  return (math.exp(x)-math.exp(-x))/(math.exp(x)+ math.exp(-x))

tanh(100) # output 1.0

tanh(0.7)

tanh(-0.98)

def relu(x):
    return max(0,x)

relu(-100)

relu(8)

relu(-2345)

def leakyrelu(x):
    return max(0.1*x,x)

leakyrelu(-100)

leakyrelu(8)

import numpy as np

RI = np.array(([200,220,250],[68,79,105],[110,140,180],[80,85,90]))


ind= 75*RI
ind

rose = np.array([50,60,25])
lotus = np.array([10,13,5])
Sun = np.array([40,70,52])

roses = 20*rose
roses

lOTUS = 30*lotus
lOTUS

SF = 15*Sun
SF

price = np.array([20,30,15])

import numpy as np

def mean_square_error(y_true, y_pred):
    if len(y_true) != len(y_pred):
        raise ValueError("Input arrays must have the same length")
    squared_diff = (y_true - y_pred) * 2
    mse = np.mean(squared_diff)
    return mse
# Example usage:
true_values = np.array([2, 4, 5, 4, 5])
predicted_values = np.array([1.5, 3.5, 4.5, 4, 5.5])

mse_result = mean_square_error(true_values, predicted_values)
print(f"Mean Square Error: {mse_result}")


import numpy as np

def categorical_cross_entropy(y_true, y_pred):
    if y_true.shape != y_pred.shape:
        raise ValueError("Input arrays must have the same shape")
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)

    return loss
num_classes = 3
num_data_points = 4
true_labels = np.eye(num_classes)[np.random.choice(num_classes, num_data_points)]
predicted_probs = np.random.rand(num_data_points, num_classes)
predicted_probs /= np.sum(predicted_probs, axis=1, keepdims=True)  # Normalize to make them probabilities

# Calculate categorical cross-entropy
ce_loss = categorical_cross_entropy(true_labels, predicted_probs)
print(f"Categorical Cross-Entropy Loss: {ce_loss}")
